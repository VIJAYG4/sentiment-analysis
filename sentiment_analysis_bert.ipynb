{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sentiment_analysis_bert",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuWFSfVJxkmH",
        "outputId": "fdb61d82-13f8-448e-a9a1-3b93871f9985"
      },
      "source": [
        "!pip install -q -U watermark\n",
        "!pip install -qq transformers\n",
        "\n",
        "#!pip3 install ktrain\n",
        "\n",
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.18.5\n",
            "pandas 1.1.4\n",
            "torch 1.7.0+cu101\n",
            "transformers 4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arIOlyx7iH_x"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#to clean text\n",
        "import spacy\n",
        "#to plot word clouds\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xTFfbUBixypV",
        "outputId": "f1c0ecf2-cb56-45bc-8129-d01450f69e1f"
      },
      "source": [
        "df = pd.read_csv(\"financial_news_sentiment.csv\",encoding='cp437',header=None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0                                                  1\n",
              "0   neutral  According to Gran , the company has no plans t...\n",
              "1   neutral  Technopolis plans to develop in stages an area...\n",
              "2  negative  The international electronic industry company ...\n",
              "3  positive  With the new production plant the company woul...\n",
              "4  positive  According to the company 's updated strategy f..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hEdw3-rLx56F",
        "outputId": "d81dc942-8531-444d-b018-f1fb6c1ae6b8"
      },
      "source": [
        "#some df processing\n",
        "df = df.rename(columns = {0:'sentiment', 1:'finance_news'})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>finance_news</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment                                       finance_news\n",
              "0   neutral  According to Gran , the company has no plans t...\n",
              "1   neutral  Technopolis plans to develop in stages an area...\n",
              "2  negative  The international electronic industry company ...\n",
              "3  positive  With the new production plant the company woul...\n",
              "4  positive  According to the company 's updated strategy f..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK7cPL6te3kY",
        "outputId": "9a5fba76-220c-4485-b569-c2d5f469532c"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral     2879\n",
              "positive    1363\n",
              "negative     604\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqDNH2UlfplI"
      },
      "source": [
        "## Any null values and duplicates?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsV4XcVXfoOL",
        "outputId": "4ef9ac66-2859-4758-8927-e09193b1b8b4"
      },
      "source": [
        "df.isnull().sum().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j995r8sifyd0"
      },
      "source": [
        "Good. No null values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWRvtNcQf2l5"
      },
      "source": [
        "Duplicates?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKDgaNm_f_Zh",
        "outputId": "736ce003-9d7c-4149-9102-7867d1247442"
      },
      "source": [
        "df.duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONdPDe4wgFAO"
      },
      "source": [
        "#remove duplicates\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "lkHDlQbKymkL",
        "outputId": "d7245cc9-f7ea-44e2-bf56-f7643caa21d2"
      },
      "source": [
        "#class distribution\n",
        "print(\"length of original df is\", len(df))\n",
        "\n",
        "#print(\"Original class distribution\")\n",
        "#df['sentiment'].value_counts().plot(kind='bar')\n",
        "#note: tried with the original class distribution, the model isn't able to predict negative at all due to less samples.\n",
        "\n",
        "#make the class distribution equal to negative samples(smallest)\n",
        "df_negative = df[df['sentiment']=='negative']\n",
        "df_neutral=df[df['sentiment']=='neutral']\n",
        "df_positive = df[df['sentiment']=='positive']\n",
        "\n",
        "neg_samples_cnt=len(df_negative)\n",
        "df_neutral = df_neutral.sample(n=neg_samples_cnt)\n",
        "df_positive = df_positive.sample(n=neg_samples_cnt)\n",
        "\n",
        "df = pd.concat([df_negative,df_neutral,df_positive], ignore_index=True)\n",
        "print(\"length of modified df is\",len(df))\n",
        "print(\"Modified Equal class distribution\")\n",
        "df['sentiment'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of original df is 4840\n",
            "length of modified df is 1812\n",
            "Modified Equal class distribution\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8386338668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_g_e-QyhVFg"
      },
      "source": [
        "## Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO11ztIOhUKA"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "def normalise(msg):\n",
        "    \n",
        "    doc = nlp(msg)\n",
        "    res = []\n",
        "    \n",
        "    for token in doc:\n",
        "        #Removing Stop words and words out of vocabulary\n",
        "        if token.is_stop or token.is_punct or token.is_space or not(token.is_oov): \n",
        "            pass\n",
        "        else:\n",
        "            res.append(token.lemma_.lower())\n",
        "            \n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KCsjKdG8iUY_",
        "outputId": "1b234a18-4d06-48d8-8b0f-a5182a3f50ff"
      },
      "source": [
        "df['finance_news_tokens'] = df['finance_news'].apply(normalise)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>finance_news</th>\n",
              "      <th>finance_news_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "      <td>[international, electronic, industry, company,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>A tinyurl link takes users to a scamming site ...</td>\n",
              "      <td>[tinyurl, link, take, user, scamme, site, prom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Compared with the FTSE 100 index , which rose ...</td>\n",
              "      <td>[compare, ftse, 100, index, rise, 36.7, point,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>Compared with the FTSE 100 index , which rose ...</td>\n",
              "      <td>[compare, ftse, 100, index, rise, 94.9, point,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>One of the challenges in the oil production in...</td>\n",
              "      <td>[challenge, oil, production, north, sea, scale...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentiment  ...                                finance_news_tokens\n",
              "0  negative  ...  [international, electronic, industry, company,...\n",
              "1  negative  ...  [tinyurl, link, take, user, scamme, site, prom...\n",
              "2  negative  ...  [compare, ftse, 100, index, rise, 36.7, point,...\n",
              "3  negative  ...  [compare, ftse, 100, index, rise, 94.9, point,...\n",
              "4  negative  ...  [challenge, oil, production, north, sea, scale...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTc1PkTIjalW"
      },
      "source": [
        "# Visuals in each sentiment\n",
        "NOTE: Wordclouds are NOT text analytics. Instead, use 'lda' or 'most common words'(lemmatized and removing stop words) for visualisation. wordclouds are optimized to look good, so DONOT use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBIiGZisjd7k"
      },
      "source": [
        "## overall representation most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "jEIJINAIjgKi",
        "outputId": "fda9f38e-3038-4961-f9a5-c1cd76b8aa52"
      },
      "source": [
        "words_collection = Counter([item for sublist in df['finance_news_tokens'] for item in sublist])\n",
        "freq_word_df = pd.DataFrame(words_collection.most_common(15))\n",
        "freq_word_df.columns = ['frequently_used_word','count']\n",
        "\n",
        "freq_word_df.style.background_gradient(cmap='YlGnBu', low=0, high=0, axis=0, subset=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row0_col1{\n",
              "            background-color:  #081d58;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row1_col1{\n",
              "            background-color:  #40b5c4;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row2_col1{\n",
              "            background-color:  #5bc0c0;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row3_col1{\n",
              "            background-color:  #caeab4;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row4_col1{\n",
              "            background-color:  #cfecb3;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row5_col1{\n",
              "            background-color:  #d9f0b3;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row6_col1{\n",
              "            background-color:  #daf0b3;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row7_col1{\n",
              "            background-color:  #dbf1b2;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row8_col1{\n",
              "            background-color:  #dcf1b2;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row9_col1{\n",
              "            background-color:  #e6f5b2;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row10_col1,#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row11_col1{\n",
              "            background-color:  #f2fabc;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row12_col1{\n",
              "            background-color:  #feffd6;\n",
              "            color:  #000000;\n",
              "        }#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row13_col1,#T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row14_col1{\n",
              "            background-color:  #ffffd9;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >frequently_used_word</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row0_col0\" class=\"data row0 col0\" >eur</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row0_col1\" class=\"data row0 col1\" >564</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row1_col0\" class=\"data row1 col0\" >company</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row1_col1\" class=\"data row1 col1\" >360</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row2_col0\" class=\"data row2 col0\" >mn</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row2_col1\" class=\"data row2 col1\" >337</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row3_col0\" class=\"data row3 col0\" >profit</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row3_col1\" class=\"data row3 col1\" >251</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row4_col0\" class=\"data row4 col0\" >finnish</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row4_col1\" class=\"data row4 col1\" >246</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row5_col0\" class=\"data row5 col0\" >`</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row5_col1\" class=\"data row5 col1\" >233</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row6_col0\" class=\"data row6 col0\" >year</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row6_col1\" class=\"data row6 col1\" >231</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row7_col0\" class=\"data row7 col0\" >say</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row7_col1\" class=\"data row7 col1\" >229</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row8_col0\" class=\"data row8 col0\" >sale</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row8_col1\" class=\"data row8 col1\" >227</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row9_col0\" class=\"data row9 col0\" >net</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row9_col1\" class=\"data row9 col1\" >215</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row10_col0\" class=\"data row10 col0\" >m</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row10_col1\" class=\"data row10 col1\" >190</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row11_col0\" class=\"data row11 col0\" >million</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row11_col1\" class=\"data row11 col1\" >190</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row12_col0\" class=\"data row12 col0\" >2009</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row12_col1\" class=\"data row12 col1\" >157</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row13_col0\" class=\"data row13 col0\" >period</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row13_col1\" class=\"data row13 col1\" >154</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row14_col0\" class=\"data row14 col0\" >quarter</td>\n",
              "                        <td id=\"T_2b43a8d0_39a6_11eb_b269_0242ac1c0002row14_col1\" class=\"data row14 col1\" >153</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f854903e208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPtxMwC2kAH0"
      },
      "source": [
        "# word_string = \" \".join(words_collection)\n",
        "\n",
        "# wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "#                           background_color='white', \n",
        "#                       max_words=1500, \n",
        "#                       width=1000,\n",
        "#                       height=650\n",
        "#                          ).generate(word_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntrVWNnHkhI9"
      },
      "source": [
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkoDZQjck-lT"
      },
      "source": [
        "Split df based on sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uZ9HkDlCS5"
      },
      "source": [
        "df_positive = df[df['sentiment']=='positive']\n",
        "df_neutral = df[df['sentiment']=='neutral']\n",
        "df_negative = df[df['sentiment']=='negative']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s0XmICflUIY"
      },
      "source": [
        "## Positive Sentiment most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "DUJYOXBTlX1x",
        "outputId": "b314fbd7-f983-466b-c526-425569fcb2b7"
      },
      "source": [
        "words_collection = Counter([item for sublist in df_positive['finance_news_tokens'] for item in sublist])\n",
        "freq_word_df = pd.DataFrame(words_collection.most_common(15))\n",
        "freq_word_df.columns = ['frequently_used_word','count']\n",
        "\n",
        "freq_word_df.style.background_gradient(cmap='PuBuGn', low=0, high=0, axis=0, subset=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row0_col1{\n",
              "            background-color:  #014636;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row1_col1{\n",
              "            background-color:  #6eabd0;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row2_col1{\n",
              "            background-color:  #9dbad9;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row3_col1{\n",
              "            background-color:  #b1c2de;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row4_col1{\n",
              "            background-color:  #b7c5df;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row5_col1{\n",
              "            background-color:  #b9c6e0;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row6_col1{\n",
              "            background-color:  #c2cbe2;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row7_col1{\n",
              "            background-color:  #dbd8ea;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row8_col1,#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row9_col1{\n",
              "            background-color:  #e3dded;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row10_col1{\n",
              "            background-color:  #ede3f0;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row11_col1{\n",
              "            background-color:  #f1e7f3;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row12_col1{\n",
              "            background-color:  #f4ebf5;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row13_col1{\n",
              "            background-color:  #f7eef6;\n",
              "            color:  #000000;\n",
              "        }#T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row14_col1{\n",
              "            background-color:  #fff7fb;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >frequently_used_word</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row0_col0\" class=\"data row0 col0\" >eur</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row0_col1\" class=\"data row0 col1\" >196</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row1_col0\" class=\"data row1 col0\" >company</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row1_col1\" class=\"data row1 col1\" >125</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row2_col0\" class=\"data row2 col0\" >`</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row2_col1\" class=\"data row2 col1\" >112</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row3_col0\" class=\"data row3 col0\" >mn</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row3_col1\" class=\"data row3 col1\" >105</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row4_col0\" class=\"data row4 col0\" >say</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row4_col1\" class=\"data row4 col1\" >103</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row5_col0\" class=\"data row5 col0\" >year</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row5_col1\" class=\"data row5 col1\" >102</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row6_col0\" class=\"data row6 col0\" >finnish</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row6_col1\" class=\"data row6 col1\" >98</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row7_col0\" class=\"data row7 col0\" >net</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row7_col1\" class=\"data row7 col1\" >86</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row8_col0\" class=\"data row8 col0\" >sale</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row8_col1\" class=\"data row8 col1\" >81</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row9_col0\" class=\"data row9 col0\" >profit</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row9_col1\" class=\"data row9 col1\" >81</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row10_col0\" class=\"data row10 col0\" >m</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row10_col1\" class=\"data row10 col1\" >75</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row11_col0\" class=\"data row11 col0\" >million</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row11_col1\" class=\"data row11 col1\" >71</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row12_col0\" class=\"data row12 col0\" >mln</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row12_col1\" class=\"data row12 col1\" >68</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row13_col0\" class=\"data row13 col0\" >increase</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row13_col1\" class=\"data row13 col1\" >66</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row14_col0\" class=\"data row14 col0\" >quarter</td>\n",
              "                        <td id=\"T_2b4a3f74_39a6_11eb_b269_0242ac1c0002row14_col1\" class=\"data row14 col1\" >58</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f854903e6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoFAfNmrluXA"
      },
      "source": [
        "# word_string = \" \".join(words_collection)\n",
        "\n",
        "# wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "#                           background_color='white', \n",
        "#                       max_words=1500, \n",
        "#                       width=1000,\n",
        "#                       height=650\n",
        "#                          ).generate(word_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxxQfCyXl0cH"
      },
      "source": [
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JATJF5PRmDds"
      },
      "source": [
        "## Negative sentiment most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "-yOz3RqrmJS3",
        "outputId": "8fa23536-bd4a-4c81-de27-6b2deaf23b9c"
      },
      "source": [
        "words_collection = Counter([item for sublist in df_negative['finance_news_tokens'] for item in sublist])\n",
        "freq_word_df = pd.DataFrame(words_collection.most_common(15))\n",
        "freq_word_df.columns = ['frequently_used_word','count']\n",
        "\n",
        "freq_word_df.style.background_gradient(cmap='PuBuGn', low=0, high=0, axis=0, subset=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row0_col1{\n",
              "            background-color:  #014636;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row1_col1{\n",
              "            background-color:  #4397c4;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row2_col1{\n",
              "            background-color:  #b7c5df;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row3_col1{\n",
              "            background-color:  #ede3f0;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row4_col1{\n",
              "            background-color:  #f0e6f2;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row5_col1{\n",
              "            background-color:  #f0e7f2;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row6_col1,#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row7_col1{\n",
              "            background-color:  #f1e7f3;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row8_col1{\n",
              "            background-color:  #f7eef7;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row9_col1{\n",
              "            background-color:  #f8eff7;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row10_col1,#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row11_col1,#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row12_col1{\n",
              "            background-color:  #faf2f8;\n",
              "            color:  #000000;\n",
              "        }#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row13_col1,#T_2b4f9442_39a6_11eb_b269_0242ac1c0002row14_col1{\n",
              "            background-color:  #fff7fb;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >frequently_used_word</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row0_col0\" class=\"data row0 col0\" >eur</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row0_col1\" class=\"data row0 col1\" >325</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row1_col0\" class=\"data row1 col0\" >mn</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row1_col1\" class=\"data row1 col1\" >224</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row2_col0\" class=\"data row2 col0\" >profit</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row2_col1\" class=\"data row2 col1\" >159</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row3_col0\" class=\"data row3 col0\" >company</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row3_col1\" class=\"data row3 col1\" >108</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row4_col0\" class=\"data row4 col0\" >net</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row4_col1\" class=\"data row4 col1\" >104</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row5_col0\" class=\"data row5 col0\" >year</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row5_col1\" class=\"data row5 col1\" >103</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row6_col0\" class=\"data row6 col0\" >sale</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row6_col1\" class=\"data row6 col1\" >102</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row7_col0\" class=\"data row7 col0\" >finnish</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row7_col1\" class=\"data row7 col1\" >102</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row8_col0\" class=\"data row8 col0\" >period</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row8_col1\" class=\"data row8 col1\" >91</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row9_col0\" class=\"data row9 col0\" >quarter</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row9_col1\" class=\"data row9 col1\" >90</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row10_col0\" class=\"data row10 col0\" >2009</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row10_col1\" class=\"data row10 col1\" >86</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row11_col0\" class=\"data row11 col0\" >m</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row11_col1\" class=\"data row11 col1\" >86</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row12_col0\" class=\"data row12 col0\" >2008</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row12_col1\" class=\"data row12 col1\" >86</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row13_col0\" class=\"data row13 col0\" >say</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row13_col1\" class=\"data row13 col1\" >78</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row14_col0\" class=\"data row14 col0\" >million</td>\n",
              "                        <td id=\"T_2b4f9442_39a6_11eb_b269_0242ac1c0002row14_col1\" class=\"data row14 col1\" >78</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f854903e6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hh78ySWmQeL"
      },
      "source": [
        "# word_string = \" \".join(words_collection)\n",
        "\n",
        "# wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "#                           background_color='white', \n",
        "#                       max_words=1500, \n",
        "#                       width=1000,\n",
        "#                       height=650\n",
        "#                          ).generate(word_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fasKs0XmXu2"
      },
      "source": [
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ-c7TiemeyQ"
      },
      "source": [
        "## Neutral sentiment most common words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "CROz0UK4mjzy",
        "outputId": "ad5fbf5d-163e-41ae-c582-f48fc8f539b9"
      },
      "source": [
        "words_collection = Counter([item for sublist in df_neutral['finance_news_tokens'] for item in sublist])\n",
        "freq_word_df = pd.DataFrame(words_collection.most_common(15))\n",
        "freq_word_df.columns = ['frequently_used_word','count']\n",
        "\n",
        "freq_word_df.style.background_gradient(cmap='PuBuGn', low=0, high=0, axis=0, subset=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row0_col1{\n",
              "            background-color:  #014636;\n",
              "            color:  #f1f1f1;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row1_col1{\n",
              "            background-color:  #9dbad9;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row2_col1{\n",
              "            background-color:  #c9cee4;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row3_col1{\n",
              "            background-color:  #e7dfee;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row4_col1{\n",
              "            background-color:  #e8e0ef;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row5_col1{\n",
              "            background-color:  #ebe1f0;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row6_col1{\n",
              "            background-color:  #ede3f1;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row7_col1{\n",
              "            background-color:  #f0e7f2;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row8_col1,#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row9_col1{\n",
              "            background-color:  #f2e9f3;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row10_col1{\n",
              "            background-color:  #f5ecf5;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row11_col1{\n",
              "            background-color:  #fef6fa;\n",
              "            color:  #000000;\n",
              "        }#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row12_col1,#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row13_col1,#T_2b55abb6_39a6_11eb_b269_0242ac1c0002row14_col1{\n",
              "            background-color:  #fff7fb;\n",
              "            color:  #000000;\n",
              "        }</style><table id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >frequently_used_word</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row0_col0\" class=\"data row0 col0\" >company</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row0_col1\" class=\"data row0 col1\" >127</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row1_col0\" class=\"data row1 col0\" >`</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row1_col1\" class=\"data row1 col1\" >71</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row2_col0\" class=\"data row2 col0\" >share</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row2_col1\" class=\"data row2 col1\" >60</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row3_col0\" class=\"data row3 col0\" >business</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row3_col1\" class=\"data row3 col1\" >49</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row4_col0\" class=\"data row4 col0\" >say</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row4_col1\" class=\"data row4 col1\" >48</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row5_col0\" class=\"data row5 col0\" >service</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row5_col1\" class=\"data row5 col1\" >47</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row6_col0\" class=\"data row6 col0\" >finnish</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row6_col1\" class=\"data row6 col1\" >46</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row7_col0\" class=\"data row7 col0\" >sale</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row7_col1\" class=\"data row7 col1\" >44</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row8_col0\" class=\"data row8 col0\" >eur</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row8_col1\" class=\"data row8 col1\" >43</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row9_col0\" class=\"data row9 col0\" >new</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row9_col1\" class=\"data row9 col1\" >43</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row10_col0\" class=\"data row10 col0\" >million</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row10_col1\" class=\"data row10 col1\" >41</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row11_col0\" class=\"data row11 col0\" >group</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row11_col1\" class=\"data row11 col1\" >36</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row12_col0\" class=\"data row12 col0\" >include</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row12_col1\" class=\"data row12 col1\" >35</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row13_col0\" class=\"data row13 col0\" >finland</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row13_col1\" class=\"data row13 col1\" >35</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row14_col0\" class=\"data row14 col0\" >market</td>\n",
              "                        <td id=\"T_2b55abb6_39a6_11eb_b269_0242ac1c0002row14_col1\" class=\"data row14 col1\" >35</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f854903e908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goMfSDismnal"
      },
      "source": [
        "# word_string = \" \".join(words_collection)\n",
        "\n",
        "# wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "#                           background_color='white', \n",
        "#                       max_words=1500, \n",
        "#                       width=1000,\n",
        "#                       height=650\n",
        "#                          ).generate(word_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLX2qcoQmqNk"
      },
      "source": [
        "# plt.figure(figsize=(20,10))\n",
        "# plt.imshow(wordcloud)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce-13SBLV2Er"
      },
      "source": [
        "class_names = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXO003ldnRYE"
      },
      "source": [
        "convert sentiment into numbers and join the finance news tokens list into string before sending it to BERT for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EmINOmIAzlxW",
        "outputId": "697bc2cc-7b42-48e4-ae28-0cc19d201e48"
      },
      "source": [
        "#convert sentiment string to target\n",
        "#target = 0:negative, 1: neutral, 2: positive\n",
        "def sentiment_to_label(sentiment):\n",
        "  if sentiment=='positive':\n",
        "    return 2\n",
        "  elif sentiment=='neutral':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "df['target']=df['sentiment'].apply(sentiment_to_label)\n",
        "\n",
        "#combine tokens into string\n",
        "df['finance_news_cleaned']=df['finance_news_tokens'].apply(lambda tokens : \" \".join(tokens))\n",
        "\n",
        "#keep only relevant columns\n",
        "keep_columns = ['target', 'finance_news_cleaned']\n",
        "df=df[keep_columns]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>finance_news_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>international electronic industry company elco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>tinyurl link take user scamme site promise use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>compare ftse 100 index rise 36.7 point 0.6 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>compare ftse 100 index rise 94.9 point 1.6 day...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>challenge oil production north sea scale forma...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target                               finance_news_cleaned\n",
              "0       0  international electronic industry company elco...\n",
              "1       0  tinyurl link take user scamme site promise use...\n",
              "2       0  compare ftse 100 index rise 36.7 point 0.6 day...\n",
              "3       0  compare ftse 100 index rise 94.9 point 1.6 day...\n",
              "4       0  challenge oil production north sea scale forma..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh6xPm6n2aVE"
      },
      "source": [
        "# load BERT model tokenizer and encode_plus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRpx6-S201q8"
      },
      "source": [
        "#load bert base model that is case senstitive\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PBjMDyq1ED-"
      },
      "source": [
        "#maximum length of tokens fed into bert model\n",
        "MAX_LEN = 160\n",
        "\n",
        "class TwitterDataset():\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    #using encode_plus to encode the reviews \n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKbTj3zU2V5K"
      },
      "source": [
        "# train-val-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxXPA9tZ1iqk"
      },
      "source": [
        "#split dataset into 60-20-20-train-val-test dataset\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_temp = train_test_split(df, test_size=0.4,train_size=0.6,stratify=df['target'],shuffle=True,random_state=42)\n",
        "df_test, df_val = train_test_split(df_temp, test_size = 0.5,train_size =0.5, stratify=df_temp['target'],shuffle=True,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nSnUmmLbODTR",
        "outputId": "cbbf7c72-ea0f-4b16-d328-fec6acc0c9d2"
      },
      "source": [
        "df_train['target'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f85483f9518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge2_EUOw2HTH",
        "outputId": "20bea228-667c-42c9-b3ec-78f0fc1f50f7"
      },
      "source": [
        "print(\"length of train dataset is \", len(df_train))\n",
        "print(\"length of val dataset is \", len(df_val))\n",
        "print(\"length of test dataset is \", len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of train dataset is  1087\n",
            "length of val dataset is  363\n",
            "length of test dataset is  362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuUUxWqR2Ti8"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghvk8cV-2JRY"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = TwitterDataset(\n",
        "    reviews=df.finance_news_cleaned.to_numpy(),\n",
        "    targets=df.target.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  \n",
        "  #split data into batches\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4QnDylj2QSC"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMcn8k13CD3"
      },
      "source": [
        "# Let's now load the basic [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and build our sentiment classifier on top of it. Load the model using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH-yUyeH2gTK"
      },
      "source": [
        "from transformers import BertModel\n",
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raGUsX2j3GgR"
      },
      "source": [
        "import torch.nn as nn\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    out = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(out[1])\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTmWZkpfVcc6"
      },
      "source": [
        "Note that our sentiment classifier takes the BERT backbone and adds a dropout layer (for regularization) and a linear dense layer, which we train using cross-entropy. Let's create an instance and move it to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ays9lfYpVgBA"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8jsbHWC30QX"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUYahM1L4ucj"
      },
      "source": [
        "To train the model, we will use the AdamW optimizer and a linear learning-rate scheduler with no warmup steps, along with the cross-entropy loss. Five epochs (full passes through the training data should be enough) should be enough, but you can experiment with more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxlyiAdf31Ju"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "EPOCHS = 10\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.nn import functional as F\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRWme9t744Je"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  #TODO: freeze the pre-trained BERT layers and train only the classifier\n",
        "  #option 1: code snippet throws errror though it has been accepted on github\n",
        "  #for param in model.bert.bert.parameters():\n",
        "  #  param.requires_grad = False\n",
        "  \n",
        "  #option 2: code snippet works fine but throws error during training.\n",
        "  '''\n",
        "  for name, param in model.named_parameters():\n",
        "\t  if 'classifier' not in name: # classifier layer\n",
        "\t\t  param.requires_grad = False\n",
        "  '''\n",
        "\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "    targets = batch['targets']\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    targets = targets.to(device)\n",
        "    \n",
        "    \n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "    )\n",
        "    #print(\"outputs is \", outputs)\n",
        "    #NOTE: donot include softmax function in training as pytorch automatically \n",
        "    #does that for you. include only in testing.\n",
        "\n",
        "    #print(\"softmaxed outputs is \",outputs)\n",
        "    #calculate y_pred\n",
        "    _, y_pred = torch.max(outputs, dim=1)\n",
        "    \n",
        "    #calculate loss\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    temp = 0\n",
        "\n",
        "    for elem in range(len(targets)):\n",
        "      if targets[elem] == y_pred[elem]:\n",
        "        temp += 1\n",
        "\n",
        "    correct_predictions += temp\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIFwViXE6LVj"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  with torch.no_grad():\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    # TODO: Q9. Reproduce the above code but only evaluate the model (without any weight updates).\n",
        "\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids']\n",
        "      attention_mask = batch['attention_mask']\n",
        "      targets = batch['targets']\n",
        "\n",
        "      input_ids = input_ids.to(device)\n",
        "      attention_mask = attention_mask.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      \n",
        "      outputs = F.softmax(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      _, y_pred = torch.max(outputs, dim=1)\n",
        "      \n",
        "      temp = 0\n",
        "\n",
        "      for elem in range(len(targets)):\n",
        "        if targets[elem] == y_pred[elem]:\n",
        "          temp += 1\n",
        "\n",
        "      correct_predictions += temp\n",
        "      losses.append(loss.item())\n",
        "\n",
        "\n",
        "  return correct_predictions / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOHMrcry6QRC",
        "outputId": "7614b79b-fa5a-478c-c85e-66b3de9cec0f"
      },
      "source": [
        "#calculate time taken for the cell - pretty nice feature huh? \n",
        "%%time\n",
        "from collections import defaultdict\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  # TODO: Q10. Complete the code below to track train and test accuracy.losses\n",
        "  # Each data loader has 16 samples, so total number of examples trained / evaluated will be size of data loader * 16\n",
        "\n",
        "  train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(train_data_loader) * 16)\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(model, test_data_loader, loss_fn, device, len(test_data_loader) * 16)\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.9284909713794204 accuracy 0.5542279411764706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.8145585811656454 accuracy 0.7717391304347826\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.49611784298630324 accuracy 0.8079044117647058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7491931707962699 accuracy 0.8097826086956522\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2907795099377194 accuracy 0.8988970588235294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7420859958814539 accuracy 0.7880434782608695\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.1563959087156143 accuracy 0.9503676470588235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7384558553281038 accuracy 0.7989130434782609\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0986700742984848 accuracy 0.9733455882352942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7498988664668539 accuracy 0.7934782608695652\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.07679370743212263 accuracy 0.9788602941176471\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7577176819676938 accuracy 0.782608695652174\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.041379539932891285 accuracy 0.9898897058823529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.745050873445428 accuracy 0.7934782608695652\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.02327931522792342 accuracy 0.9935661764705882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7446040562961412 accuracy 0.7934782608695652\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.024365569546181874 accuracy 0.9935661764705882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7399396300315857 accuracy 0.7989130434782609\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.007369541068756542 accuracy 0.9981617647058824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.7379379298376001 accuracy 0.7989130434782609\n",
            "\n",
            "CPU times: user 3min 27s, sys: 2min 9s, total: 5min 36s\n",
            "Wall time: 5min 43s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s6KhxJXcwir"
      },
      "source": [
        "Note that we're storing the best model, indicated by the highest validation accuracy.\n",
        "\n",
        "Plot train and validation accuracy as a function of epoch count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "kXk7ZPIJYIXS",
        "outputId": "0b62ab25-385a-4f6b-a97e-271752d9498d"
      },
      "source": [
        "# TODO: Q11. Plot train/validation accuracies.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('Model Accuracies')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['train', 'val'], loc = 'upper left')\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMuOx0j7c1Ee",
        "outputId": "85889347-16a6-491d-a1a4-f075445fee61"
      },
      "source": [
        "train_acc, val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9981617647058824, 0.7989130434782609)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKh47ryIr2ua"
      },
      "source": [
        "You might try to fine-tune the parameters (learning rate, batch size) a bit more if accuracy is not good enough.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEr0UTL6r9XH"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGhnNrxCsCMm"
      },
      "source": [
        "We'll define a helper function to get the predictions from our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Myzuetir3bI"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTno3o-tshMT"
      },
      "source": [
        "This is similar to the evaluation function, except that we're storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNWcv0YzsY36",
        "outputId": "f551ece2-02d1-4ac9-df00-cef2db91e843"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqrCrEGgstnb"
      },
      "source": [
        "Let us compare true sentiment vs predicted sentiment by plotting a confusion matrix of `y_test` vs `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVPFubNYsm5O",
        "outputId": "0336e0f6-8f2c-4764-caaa-f48ef7334331"
      },
      "source": [
        "cnt = 0\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i] == y_pred[i]:\n",
        "    cnt += 1\n",
        "\n",
        "cnt = cnt / len(y_test)\n",
        "print(f'Test Accuracy is {cnt}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy is 0.8121546961325967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2cZr1vMs08v",
        "outputId": "6acfacad-0f12-4ed2-9dc9-4ee08f1dd290"
      },
      "source": [
        "# TODO. Q12. Plot the 3x3 confusion matrix and show that the model finds it a bit difficult to classify neutral reviews.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "#print(cm)\n",
        "\n",
        "conf_mat_df = pd.DataFrame(cm)\n",
        "print(conf_mat_df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0   1   2\n",
            "0  107   5   8\n",
            "1    7  95  19\n",
            "2   10  19  92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wklQMkygs-LF",
        "outputId": "42d3ffe8-697c-409c-db56-8b4fed2457a9"
      },
      "source": [
        "#Other Evaluation metrics\n",
        "from sklearn.metrics import classification_report\n",
        "target_names = ['negative', 'neutral', 'positive']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.89      0.88       120\n",
            "     neutral       0.80      0.79      0.79       121\n",
            "    positive       0.77      0.76      0.77       121\n",
            "\n",
            "    accuracy                           0.81       362\n",
            "   macro avg       0.81      0.81      0.81       362\n",
            "weighted avg       0.81      0.81      0.81       362\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZVfBjSdvDhL"
      },
      "source": [
        "#Final Notes:\n",
        "Ways to improve:\n",
        "1. Increase no of epochs, play with hyperparameter values like learning rate, batch size etc. \n",
        "2. The input feature vector for the BERT model is of length 160 currently. Play around with this length as well. \n",
        "3. code change required: currently uses 'review text' column in get_predictions function. this is from google play reviews dataset. do appropriate changes later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALkLI7292OXL"
      },
      "source": [
        "# Stock Movement prediction using sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "sjPlNd65teAS",
        "outputId": "06d004f0-ac97-4445-cc58-f31d736a2ebb"
      },
      "source": [
        "#read df\n",
        "aapl_df = pd.read_csv('AAPL_test_dataset.csv')\n",
        "aapl_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-d7ab4364b485>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#read df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maapl_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AAPL_test_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maapl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AAPL_test_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pltj7-Dn3EyZ"
      },
      "source": [
        "Follow same data processing steps as training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxX_Derl3d5S"
      },
      "source": [
        "Any null values and duplicates?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEdcYkmU2u4m"
      },
      "source": [
        "aapl_df.isnull().sum().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWHXRstv3irg"
      },
      "source": [
        "No null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuKY3iKC3hd6"
      },
      "source": [
        "aapl_df.duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyYguB0D3nYv"
      },
      "source": [
        "#remove duplicates\n",
        "aapl_df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwETeYIB3zcc"
      },
      "source": [
        "Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOFBhYB03sWP"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "def normalise(msg):\n",
        "    \n",
        "    doc = nlp(msg)\n",
        "    res = []\n",
        "    \n",
        "    for token in doc:\n",
        "        #Removing Stop words and words out of vocabulary\n",
        "        if token.is_stop or token.is_punct or token.is_space or not(token.is_oov): \n",
        "            pass\n",
        "        else:\n",
        "            res.append(token.lemma_.lower())\n",
        "            \n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkjagyeB35Qi"
      },
      "source": [
        "aapl_df['news_tokens'] = aapl_df['News'].apply(normalise)\n",
        "aapl_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1s1hLjF4E_5"
      },
      "source": [
        "#convert Label column to lowercase and convert Label to target\n",
        "aapl_df['Label']=aapl_df['Label'].str.lower()\n",
        "aapl_df['target']=aapl_df['Label'].apply(sentiment_to_label)\n",
        "\n",
        "#combine news_tokens into string\n",
        "aapl_df['news_cleaned']=aapl_df['news_tokens'].apply(lambda tokens : \" \".join(tokens))\n",
        "\n",
        "aapl_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTOtzMsX5VlF"
      },
      "source": [
        "def create_aapl_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = TwitterDataset(\n",
        "    reviews=df.news_cleaned.to_numpy(),\n",
        "    targets=df.target.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "  \n",
        "  #split data into batches\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )\n",
        "\n",
        "#create aapl_test_data_loader to predict stock prices movement\n",
        "aapl_test_data_loader = create_aapl_data_loader(aapl_df, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUdGfeQY9Kql"
      },
      "source": [
        "## Get predictions for aapl_test_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ7gHQkz6EzK"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  aapl_test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQeWnh2t9Q7S"
      },
      "source": [
        "cnt = 0\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i] == y_pred[i]:\n",
        "    cnt += 1\n",
        "\n",
        "cnt = cnt / len(y_test)\n",
        "print(f'Test Accuracy is {cnt}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dkrqH869fZ3"
      },
      "source": [
        "# TODO. Q12. Plot the 3x3 confusion matrix and show that the model finds it a bit difficult to classify neutral reviews.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "#print(cm)\n",
        "\n",
        "conf_mat_df = pd.DataFrame(cm)\n",
        "print(conf_mat_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDC6vEHe90v4"
      },
      "source": [
        "#Other Evaluation metrics\n",
        "from sklearn.metrics import classification_report\n",
        "target_names = ['negative', 'neutral', 'positive']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_58W1OUs_Cu1"
      },
      "source": [
        "def prediction_to_sentiment(y_pred):\n",
        "  if y_pred==0:\n",
        "    return 'negative'\n",
        "  elif y_pred==1:\n",
        "    return 'neutral'\n",
        "  else:\n",
        "    return 'positive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0YDiBAU-AlS"
      },
      "source": [
        "aapl_df['prediction']=y_pred \n",
        "aapl_df['prediction']=aapl_df['prediction'].apply(prediction_to_sentiment)\n",
        "print(aapl_df.head())\n",
        "\n",
        "#store the predictions in csv file\n",
        "aapl_df.to_csv('aapl_test_dataset_predictions.csv',index=False,header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a6xR3kWWTM8"
      },
      "source": [
        "# Testing 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FYX_qVyWXv9"
      },
      "source": [
        "## Using Dow Jones Industrial Average dataset for testing.\n",
        "Source: https://www.kaggle.com/aaron7sun/stocknews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wJGtE8GAORs"
      },
      "source": [
        "djia_news_df = pd.read_csv('DJIA_top25_news.csv')\n",
        "djia_stock_df = pd.read_csv('DJIA_stock_price.csv')\n",
        "\n",
        "print(djia_news_df.head())\n",
        "print(djia_stock_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOMUmlHmHOYu"
      },
      "source": [
        "print(\"shape of DJIA news df is\",djia_news_df.shape)\n",
        "print(\"shape of DJIA stock df is\",djia_stock_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wqAo6j5HkMF"
      },
      "source": [
        "#Merge DJIA news and stock dfs into one\n",
        "djia_df = djia_news_df.merge(djia_stock_df, how='inner', on='Date', left_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2pGlqcdYEpf"
      },
      "source": [
        "#Show merged df\n",
        "djia_df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QC79Ek_IC7n"
      },
      "source": [
        "#Combine top news headlines into one. \n",
        "combined_news = []\n",
        "for row in range(len(djia_df)):\n",
        "  combined_news.append(\" \".join( [str(x) for x in djia_df.iloc[row,2:27]] ))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tao_5lyRXnDe"
      },
      "source": [
        "#print sample combined_news\n",
        "combined_news[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2jdnsacYpGf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}